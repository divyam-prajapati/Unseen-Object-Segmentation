{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 240000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.io import read_image\n",
    "from prettytable import PrettyTable\n",
    "from segment_anything import sam_model_registry\n",
    "import os\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from skimage.transform import resize\n",
    "\n",
    "NUM_VIEWS_PER_SCENE = 6\n",
    "TOD_filepath = 'C:/dataset/TOD/training_set/'\n",
    "TOD_PROCESSED = 'C:/dataset/TOD/preprocessed/'\n",
    "\n",
    "# Utils Functions\n",
    "def get_bounding_boxes(mask):\n",
    "    obj_ids = torch.unique(mask)\n",
    "    obj_ids = obj_ids[1:]\n",
    "    masks = mask == obj_ids[:, None, None]\n",
    "    boxes = masks_to_boxes(masks).detach().numpy()\n",
    "    if boxes.all()==None:\n",
    "        boxes=np.array([])\n",
    "    return boxes\n",
    "\n",
    "def array_to_tensor(array):\n",
    "    \"\"\" Converts a numpy.ndarray (N x H x W x C) to a torch.FloatTensor of shape (N x C x H x W)\n",
    "        OR\n",
    "        converts a nump.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W)\n",
    "    \"\"\"\n",
    "    if array.ndim == 4: # NHWC\n",
    "        tensor = torch.from_numpy(array).permute(0,3,1,2).float()\n",
    "    elif array.ndim == 3: # HWC\n",
    "        tensor = torch.from_numpy(array).permute(2,0,1).float()\n",
    "    else: # everything else\n",
    "        tensor = torch.from_numpy(array).float()\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def show_mem(device):\n",
    "    return f\"gpu used {torch.cuda.max_memory_allocated(device)/ (1024 ** 3):.02} GB memory\"\n",
    "\n",
    "# Visualization & Ploting Functions\n",
    "def visualize_segmentation(im, masks, nc=None):\n",
    "    \"\"\" Visualize segmentations nicely. Based on code from:\n",
    "        https://github.com/roytseng-tw/Detectron.pytorch/blob/master/lib/utils/vis.py\n",
    "\n",
    "        @param im: a [H x W x 3] RGB image. numpy array of dtype np.uint8\n",
    "        @param masks: a [H x W] numpy array of dtype np.uint8 with values in {0, ..., nc-1}\n",
    "        @param nc: total number of colors. If None, this will be inferred by masks\n",
    "\n",
    "        @return: a [H x W x 3] numpy array of dtype np.uint8\n",
    "    \"\"\" \n",
    "    masks = masks.astype(int)\n",
    "    im = im.copy()\n",
    "\n",
    "    # Generate color mask\n",
    "    if nc is None:\n",
    "        NUM_COLORS = masks.max() + 1\n",
    "    else:\n",
    "        NUM_COLORS = nc\n",
    "\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    colors = [cm(1. * i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "\n",
    "    # Mask\n",
    "    imgMask = np.zeros(im.shape)\n",
    "\n",
    "\n",
    "    # Draw color masks\n",
    "    for i in np.unique(masks):\n",
    "        if i == 0: # background\n",
    "            continue\n",
    "\n",
    "        # Get the color mask\n",
    "        color_mask = np.array(colors[i][:3])\n",
    "        w_ratio = .4\n",
    "        for c in range(3):\n",
    "            color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\n",
    "        e = (masks == i)\n",
    "\n",
    "        # Add to the mask\n",
    "        imgMask[e] = color_mask\n",
    "\n",
    "    # Add the mask to the image\n",
    "    imgMask = (imgMask * 255).round().astype(np.uint8)\n",
    "    im = cv2.addWeighted(im, 0.5, imgMask, 0.5, 0.0)\n",
    "\n",
    "\n",
    "    # Draw mask contours\n",
    "    for i in np.unique(masks):\n",
    "        if i == 0: # background\n",
    "            continue\n",
    "\n",
    "        # Get the color mask\n",
    "        color_mask = np.array(colors[i][:3])\n",
    "        w_ratio = .4\n",
    "        for c in range(3):\n",
    "            color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\n",
    "        e = (masks == i)\n",
    "\n",
    "        # Find contours\n",
    "        contour, hier = cv2.findContours(\n",
    "            e.astype(np.uint8).copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "        # Plot the nice outline\n",
    "        for c in contour:\n",
    "            cv2.drawContours(im, contour, -1, (255,255,255), 2)\n",
    "\n",
    "    return im\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "# # Ref: https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "def show_bbox_and_mask(rgb_img, bbox, mask_img):\n",
    "    seg_img=visualize_segmentation(rgb_img,mask_img)\n",
    "    fig = plt.figure(figsize=(10, 10)) \n",
    "    \n",
    "    fig.add_subplot(1, 3, 1) \n",
    "    plt.imshow(rgb_img)\n",
    "    for box in bbox:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.title(\"BBOX\")\n",
    "\n",
    "    fig.add_subplot(1, 3, 2) \n",
    "    plt.imshow(mask_img)\n",
    "    plt.axis('on')\n",
    "    plt.title(\"MASK\")\n",
    "\n",
    "    fig.add_subplot(1, 3, 3) \n",
    "    plt.imshow(seg_img)\n",
    "    plt.axis('on')\n",
    "    plt.title(\"Segmentation\")\n",
    "    plt.show()\n",
    "\n",
    "scene_dirs = sorted(glob.glob(TOD_filepath + '*/'))\n",
    "num_samples = len(scene_dirs)*NUM_VIEWS_PER_SCENE\n",
    "print(len(scene_dirs), num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 0.0 GB memory\n",
      "+-----------------------------------------------------------------------------+------------+\n",
      "|                                   Modules                                   | Parameters |\n",
      "+-----------------------------------------------------------------------------+------------+\n",
      "|                           image_encoder.pos_embed                           |  5242880   |\n",
      "|                    image_encoder.patch_embed.proj.weight                    |   983040   |\n",
      "|                     image_encoder.patch_embed.proj.bias                     |    1280    |\n",
      "|                     image_encoder.blocks.0.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.0.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.0.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.0.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.0.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.0.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.0.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.0.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.0.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.0.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.0.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.0.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.0.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.0.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.1.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.1.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.1.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.1.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.1.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.1.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.1.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.1.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.1.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.1.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.1.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.1.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.1.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.1.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.2.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.2.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.2.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.2.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.2.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.2.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.2.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.2.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.2.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.2.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.2.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.2.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.2.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.2.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.3.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.3.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.3.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.3.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.3.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.3.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.3.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.3.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.3.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.3.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.3.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.3.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.3.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.3.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.4.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.4.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.4.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.4.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.4.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.4.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.4.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.4.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.4.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.4.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.4.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.4.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.4.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.4.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.5.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.5.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.5.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.5.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.5.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.5.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.5.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.5.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.5.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.5.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.5.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.5.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.5.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.5.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.6.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.6.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.6.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.6.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.6.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.6.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.6.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.6.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.6.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.6.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.6.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.6.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.6.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.6.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.7.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.7.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.7.attn.rel_pos_h                    |   10160    |\n",
      "|                    image_encoder.blocks.7.attn.rel_pos_w                    |   10160    |\n",
      "|                    image_encoder.blocks.7.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.7.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.7.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.7.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.7.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.7.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.7.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.7.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.7.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.7.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.8.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.8.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.8.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.8.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.8.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.8.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.8.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.8.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.8.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.8.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.8.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.8.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.8.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.8.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.9.norm1.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.9.norm1.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.9.attn.rel_pos_h                    |    2160    |\n",
      "|                    image_encoder.blocks.9.attn.rel_pos_w                    |    2160    |\n",
      "|                    image_encoder.blocks.9.attn.qkv.weight                   |  4915200   |\n",
      "|                     image_encoder.blocks.9.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.9.attn.proj.weight                   |  1638400   |\n",
      "|                    image_encoder.blocks.9.attn.proj.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.9.norm2.weight                     |    1280    |\n",
      "|                      image_encoder.blocks.9.norm2.bias                      |    1280    |\n",
      "|                    image_encoder.blocks.9.mlp.lin1.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.9.mlp.lin1.bias                    |    5120    |\n",
      "|                    image_encoder.blocks.9.mlp.lin2.weight                   |  6553600   |\n",
      "|                     image_encoder.blocks.9.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.10.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.10.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.10.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.10.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.10.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.10.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.10.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.10.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.10.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.10.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.10.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.10.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.10.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.10.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.11.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.11.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.11.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.11.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.11.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.11.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.11.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.11.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.11.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.11.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.11.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.11.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.11.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.11.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.12.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.12.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.12.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.12.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.12.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.12.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.12.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.12.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.12.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.12.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.12.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.12.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.12.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.12.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.13.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.13.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.13.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.13.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.13.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.13.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.13.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.13.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.13.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.13.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.13.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.13.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.13.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.13.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.14.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.14.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.14.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.14.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.14.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.14.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.14.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.14.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.14.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.14.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.14.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.14.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.14.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.14.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.15.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.15.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.15.attn.rel_pos_h                   |   10160    |\n",
      "|                    image_encoder.blocks.15.attn.rel_pos_w                   |   10160    |\n",
      "|                   image_encoder.blocks.15.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.15.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.15.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.15.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.15.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.15.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.15.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.15.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.15.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.15.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.16.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.16.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.16.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.16.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.16.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.16.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.16.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.16.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.16.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.16.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.16.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.16.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.16.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.16.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.17.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.17.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.17.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.17.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.17.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.17.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.17.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.17.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.17.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.17.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.17.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.17.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.17.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.17.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.18.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.18.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.18.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.18.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.18.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.18.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.18.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.18.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.18.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.18.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.18.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.18.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.18.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.18.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.19.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.19.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.19.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.19.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.19.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.19.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.19.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.19.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.19.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.19.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.19.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.19.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.19.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.19.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.20.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.20.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.20.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.20.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.20.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.20.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.20.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.20.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.20.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.20.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.20.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.20.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.20.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.20.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.21.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.21.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.21.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.21.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.21.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.21.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.21.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.21.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.21.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.21.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.21.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.21.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.21.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.21.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.22.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.22.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.22.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.22.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.22.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.22.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.22.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.22.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.22.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.22.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.22.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.22.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.22.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.22.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.23.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.23.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.23.attn.rel_pos_h                   |   10160    |\n",
      "|                    image_encoder.blocks.23.attn.rel_pos_w                   |   10160    |\n",
      "|                   image_encoder.blocks.23.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.23.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.23.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.23.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.23.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.23.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.23.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.23.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.23.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.23.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.24.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.24.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.24.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.24.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.24.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.24.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.24.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.24.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.24.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.24.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.24.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.24.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.24.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.24.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.25.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.25.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.25.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.25.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.25.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.25.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.25.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.25.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.25.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.25.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.25.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.25.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.25.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.25.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.26.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.26.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.26.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.26.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.26.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.26.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.26.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.26.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.26.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.26.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.26.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.26.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.26.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.26.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.27.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.27.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.27.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.27.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.27.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.27.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.27.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.27.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.27.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.27.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.27.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.27.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.27.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.27.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.28.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.28.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.28.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.28.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.28.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.28.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.28.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.28.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.28.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.28.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.28.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.28.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.28.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.28.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.29.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.29.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.29.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.29.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.29.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.29.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.29.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.29.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.29.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.29.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.29.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.29.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.29.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.29.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.30.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.30.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.30.attn.rel_pos_h                   |    2160    |\n",
      "|                    image_encoder.blocks.30.attn.rel_pos_w                   |    2160    |\n",
      "|                   image_encoder.blocks.30.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.30.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.30.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.30.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.30.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.30.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.30.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.30.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.30.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.30.mlp.lin2.bias                    |    1280    |\n",
      "|                     image_encoder.blocks.31.norm1.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.31.norm1.bias                     |    1280    |\n",
      "|                    image_encoder.blocks.31.attn.rel_pos_h                   |   10160    |\n",
      "|                    image_encoder.blocks.31.attn.rel_pos_w                   |   10160    |\n",
      "|                   image_encoder.blocks.31.attn.qkv.weight                   |  4915200   |\n",
      "|                    image_encoder.blocks.31.attn.qkv.bias                    |    3840    |\n",
      "|                   image_encoder.blocks.31.attn.proj.weight                  |  1638400   |\n",
      "|                    image_encoder.blocks.31.attn.proj.bias                   |    1280    |\n",
      "|                     image_encoder.blocks.31.norm2.weight                    |    1280    |\n",
      "|                      image_encoder.blocks.31.norm2.bias                     |    1280    |\n",
      "|                   image_encoder.blocks.31.mlp.lin1.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.31.mlp.lin1.bias                    |    5120    |\n",
      "|                   image_encoder.blocks.31.mlp.lin2.weight                   |  6553600   |\n",
      "|                    image_encoder.blocks.31.mlp.lin2.bias                    |    1280    |\n",
      "|                         image_encoder.neck.0.weight                         |   327680   |\n",
      "|                         image_encoder.neck.1.weight                         |    256     |\n",
      "|                          image_encoder.neck.1.bias                          |    256     |\n",
      "|                         image_encoder.neck.2.weight                         |   589824   |\n",
      "|                         image_encoder.neck.3.weight                         |    256     |\n",
      "|                          image_encoder.neck.3.bias                          |    256     |\n",
      "|                   prompt_encoder.point_embeddings.0.weight                  |    256     |\n",
      "|                   prompt_encoder.point_embeddings.1.weight                  |    256     |\n",
      "|                   prompt_encoder.point_embeddings.2.weight                  |    256     |\n",
      "|                   prompt_encoder.point_embeddings.3.weight                  |    256     |\n",
      "|                   prompt_encoder.not_a_point_embed.weight                   |    256     |\n",
      "|                   prompt_encoder.mask_downscaling.0.weight                  |     16     |\n",
      "|                    prompt_encoder.mask_downscaling.0.bias                   |     4      |\n",
      "|                   prompt_encoder.mask_downscaling.1.weight                  |     4      |\n",
      "|                    prompt_encoder.mask_downscaling.1.bias                   |     4      |\n",
      "|                   prompt_encoder.mask_downscaling.3.weight                  |    256     |\n",
      "|                    prompt_encoder.mask_downscaling.3.bias                   |     16     |\n",
      "|                   prompt_encoder.mask_downscaling.4.weight                  |     16     |\n",
      "|                    prompt_encoder.mask_downscaling.4.bias                   |     16     |\n",
      "|                   prompt_encoder.mask_downscaling.6.weight                  |    4096    |\n",
      "|                    prompt_encoder.mask_downscaling.6.bias                   |    256     |\n",
      "|                     prompt_encoder.no_mask_embed.weight                     |    256     |\n",
      "|          mask_decoder.transformer.layers.0.self_attn.q_proj.weight          |   65536    |\n",
      "|           mask_decoder.transformer.layers.0.self_attn.q_proj.bias           |    256     |\n",
      "|          mask_decoder.transformer.layers.0.self_attn.k_proj.weight          |   65536    |\n",
      "|           mask_decoder.transformer.layers.0.self_attn.k_proj.bias           |    256     |\n",
      "|          mask_decoder.transformer.layers.0.self_attn.v_proj.weight          |   65536    |\n",
      "|           mask_decoder.transformer.layers.0.self_attn.v_proj.bias           |    256     |\n",
      "|         mask_decoder.transformer.layers.0.self_attn.out_proj.weight         |   65536    |\n",
      "|          mask_decoder.transformer.layers.0.self_attn.out_proj.bias          |    256     |\n",
      "|                mask_decoder.transformer.layers.0.norm1.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.0.norm1.bias                |    256     |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias   |    128     |\n",
      "| mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight |   32768    |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias  |    256     |\n",
      "|                mask_decoder.transformer.layers.0.norm2.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.0.norm2.bias                |    256     |\n",
      "|              mask_decoder.transformer.layers.0.mlp.lin1.weight              |   524288   |\n",
      "|               mask_decoder.transformer.layers.0.mlp.lin1.bias               |    2048    |\n",
      "|              mask_decoder.transformer.layers.0.mlp.lin2.weight              |   524288   |\n",
      "|               mask_decoder.transformer.layers.0.mlp.lin2.bias               |    256     |\n",
      "|                mask_decoder.transformer.layers.0.norm3.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.0.norm3.bias                |    256     |\n",
      "|                mask_decoder.transformer.layers.0.norm4.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.0.norm4.bias                |    256     |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias   |    128     |\n",
      "| mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight |   32768    |\n",
      "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias  |    256     |\n",
      "|          mask_decoder.transformer.layers.1.self_attn.q_proj.weight          |   65536    |\n",
      "|           mask_decoder.transformer.layers.1.self_attn.q_proj.bias           |    256     |\n",
      "|          mask_decoder.transformer.layers.1.self_attn.k_proj.weight          |   65536    |\n",
      "|           mask_decoder.transformer.layers.1.self_attn.k_proj.bias           |    256     |\n",
      "|          mask_decoder.transformer.layers.1.self_attn.v_proj.weight          |   65536    |\n",
      "|           mask_decoder.transformer.layers.1.self_attn.v_proj.bias           |    256     |\n",
      "|         mask_decoder.transformer.layers.1.self_attn.out_proj.weight         |   65536    |\n",
      "|          mask_decoder.transformer.layers.1.self_attn.out_proj.bias          |    256     |\n",
      "|                mask_decoder.transformer.layers.1.norm1.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.1.norm1.bias                |    256     |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias   |    128     |\n",
      "| mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight |   32768    |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias  |    256     |\n",
      "|                mask_decoder.transformer.layers.1.norm2.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.1.norm2.bias                |    256     |\n",
      "|              mask_decoder.transformer.layers.1.mlp.lin1.weight              |   524288   |\n",
      "|               mask_decoder.transformer.layers.1.mlp.lin1.bias               |    2048    |\n",
      "|              mask_decoder.transformer.layers.1.mlp.lin2.weight              |   524288   |\n",
      "|               mask_decoder.transformer.layers.1.mlp.lin2.bias               |    256     |\n",
      "|                mask_decoder.transformer.layers.1.norm3.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.1.norm3.bias                |    256     |\n",
      "|                mask_decoder.transformer.layers.1.norm4.weight               |    256     |\n",
      "|                 mask_decoder.transformer.layers.1.norm4.bias                |    256     |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias   |    128     |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight  |   32768    |\n",
      "|   mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias   |    128     |\n",
      "| mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight |   32768    |\n",
      "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias  |    256     |\n",
      "|       mask_decoder.transformer.final_attn_token_to_image.q_proj.weight      |   32768    |\n",
      "|        mask_decoder.transformer.final_attn_token_to_image.q_proj.bias       |    128     |\n",
      "|       mask_decoder.transformer.final_attn_token_to_image.k_proj.weight      |   32768    |\n",
      "|        mask_decoder.transformer.final_attn_token_to_image.k_proj.bias       |    128     |\n",
      "|       mask_decoder.transformer.final_attn_token_to_image.v_proj.weight      |   32768    |\n",
      "|        mask_decoder.transformer.final_attn_token_to_image.v_proj.bias       |    128     |\n",
      "|      mask_decoder.transformer.final_attn_token_to_image.out_proj.weight     |   32768    |\n",
      "|       mask_decoder.transformer.final_attn_token_to_image.out_proj.bias      |    256     |\n",
      "|               mask_decoder.transformer.norm_final_attn.weight               |    256     |\n",
      "|                mask_decoder.transformer.norm_final_attn.bias                |    256     |\n",
      "|                        mask_decoder.iou_token.weight                        |    256     |\n",
      "|                       mask_decoder.mask_tokens.weight                       |    1024    |\n",
      "|                    mask_decoder.output_upscaling.0.weight                   |   65536    |\n",
      "|                     mask_decoder.output_upscaling.0.bias                    |     64     |\n",
      "|                    mask_decoder.output_upscaling.1.weight                   |     64     |\n",
      "|                     mask_decoder.output_upscaling.1.bias                    |     64     |\n",
      "|                    mask_decoder.output_upscaling.3.weight                   |    8192    |\n",
      "|                     mask_decoder.output_upscaling.3.bias                    |     32     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.0.layers.0.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.0.layers.0.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.0.layers.1.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.0.layers.1.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.0.layers.2.weight          |    8192    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.0.layers.2.bias           |     32     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.1.layers.0.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.1.layers.0.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.1.layers.1.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.1.layers.1.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.1.layers.2.weight          |    8192    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.1.layers.2.bias           |     32     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.2.layers.0.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.2.layers.0.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.2.layers.1.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.2.layers.1.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.2.layers.2.weight          |    8192    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.2.layers.2.bias           |     32     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.3.layers.0.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.3.layers.0.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.3.layers.1.weight          |   65536    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.3.layers.1.bias           |    256     |\n",
      "|           mask_decoder.output_hypernetworks_mlps.3.layers.2.weight          |    8192    |\n",
      "|            mask_decoder.output_hypernetworks_mlps.3.layers.2.bias           |     32     |\n",
      "|               mask_decoder.iou_prediction_head.layers.0.weight              |   65536    |\n",
      "|                mask_decoder.iou_prediction_head.layers.0.bias               |    256     |\n",
      "|               mask_decoder.iou_prediction_head.layers.1.weight              |   65536    |\n",
      "|                mask_decoder.iou_prediction_head.layers.1.bias               |    256     |\n",
      "|               mask_decoder.iou_prediction_head.layers.2.weight              |    1024    |\n",
      "|                mask_decoder.iou_prediction_head.layers.2.bias               |     4      |\n",
      "+-----------------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 641090608\n",
      "gpu used 2.5 GB memory\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = \"sam_vit_b_01ec64.pth\"\n",
    "# model_type = \"vit_b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(show_mem(device))\n",
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to(device)\n",
    "count_parameters(sam_model)\n",
    "print(show_mem(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 600/600 [24:17<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Ref: https://github.com/bowang-lab/MedSAM/blob/66cf4799a9ab9a8e08428a5087e73fc21b2b61cd/finetune_and_inference_tutorial_2D_dataset.ipynb\n",
    "\n",
    "START_DATA = 1500\n",
    "END_DATA = 2100\n",
    "\n",
    "# tensorboard --logdir=D:\\UTD\\Summer\\log\\sam\n",
    "# tensorboard --logdir=./log/sam\n",
    "\n",
    "with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/sam'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    ") as prof:\n",
    "    for idx in tqdm(range(START_DATA, END_DATA)):\n",
    "        \n",
    "        # Get scene directory\n",
    "        scene_idx = idx // NUM_VIEWS_PER_SCENE                                              # [0-40,000]\n",
    "        scene_dir = scene_dirs[scene_idx]       \n",
    "\n",
    "        # Get view number\n",
    "        view_num = (idx % NUM_VIEWS_PER_SCENE)+1                                            # [0-240,000]\n",
    "        \n",
    "        # get image and mask paths\n",
    "        rgb_img_filename = scene_dir + f\"rgb_{view_num:05d}.jpeg\"   \n",
    "        mask_filename = scene_dir + f\"segmentation_{view_num:05d}.png\"\n",
    "        \n",
    "        # Read image and segmentation mask \n",
    "        image = cv2.cvtColor(cv2.imread(rgb_img_filename), cv2.COLOR_BGR2RGB)               # Shape: (480, 640, 3)                   OG                                           \n",
    "        mask  = np.array(Image.open(mask_filename))                                         # (480, 640)                             OG\n",
    "        \n",
    "        original_image_size = image.shape[:2]                                                                                                 \n",
    "\n",
    "        # Genrate Bounding Boxes from segmentation image\n",
    "        boxes = get_bounding_boxes(read_image(mask_filename))                               # Shape: (*, 4) - xmin, ymin, xmax, ymax OG    \n",
    "        # boxes_torch = torch.tensor(boxes)                                                                                                     \n",
    "\n",
    "        resize_image = resize(\n",
    "            image, \n",
    "            (sam_model.image_encoder.img_size, sam_model.image_encoder.img_size),           # Shape: (1024, 1024, 3)\n",
    "            anti_aliasing=False) \n",
    "\n",
    "        # Transform input image to the proper input format and size\n",
    "        sam_transformations = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        transform_image = sam_transformations.apply_image(resize_image)                     # Shape: (1024, 1024, 3) np\n",
    "        transform_image = torch.as_tensor(transform_image)                                  # Shape: (1024, 1024, 3) torch\n",
    "        transform_image = transform_image.permute(2, 0, 1).contiguous()                     # Shape: (3, 1024, 1024) torch\n",
    "\n",
    "        # Transform boxes to proper input format\n",
    "        # transform_boxes = sam_transformations.apply_boxes_torch(boxes_torch, original_image_size) # (*, 4) but scaled to (1024, 1024) IO \n",
    "\n",
    "        # Seperate Segments out of masks \n",
    "        num_objects = boxes.shape[0]                                                        # no. of objects in the image  (*)              \n",
    "        \n",
    "        # Add Batch to the sample input \n",
    "        torch_image = torch.tensor(np.array([transform_image]))                             # Shape: (1, 3, 1024, 1024)\n",
    "        # torch_mask  = torch.tensor(np.array([gtmask])).permute(1, 0, 2, 3)                  # Shape: (*, 1, 480, 640)                 IO\n",
    "\n",
    "        # Generate Image Embedding  \n",
    "        with torch.no_grad():\n",
    "            input_image = sam_model.preprocess(torch_image.to(device))                          # Shape: (1, 3, 1024, 1024)               \n",
    "            assert input_image.shape == (1, 3, sam_model.image_encoder.img_size, sam_model.image_encoder.img_size)\n",
    "            image_embedding = sam_model.image_encoder(input_image)                          # Shape: (1, 256, 64, 64)                 IO\n",
    "\n",
    "        # Save All\n",
    "        if not os.path.exists(TOD_PROCESSED+f\"scene_{scene_idx:05d}\"):\n",
    "            os.makedirs(TOD_PROCESSED+f\"scene_{scene_idx:05d}\")\n",
    "\n",
    "        np.save(TOD_PROCESSED+f\"scene_{scene_idx:05d}/\"+f\"embeddings_{view_num:05d}\", image_embedding.cpu().numpy())  # Shape: (1, 256, 64, 64)\n",
    "        np.save(TOD_PROCESSED+f\"scene_{scene_idx:05d}/\"+f\"image_{view_num:05d}\", np.array(image))                     # Shape: (480, 640, 3) \n",
    "        np.save(TOD_PROCESSED+f\"scene_{scene_idx:05d}/\"+f\"mask_{view_num:05d}\", np.array([mask]))                     # Shape: (1, 480, 640)  \n",
    "        np.save(TOD_PROCESSED+f\"scene_{scene_idx:05d}/\"+f\"boxes_{view_num:05d}\", np.array(boxes))                     # Shape: (*, 4)\n",
    "\n",
    "        prof.step() # Need to call this at the end of each step to notify profiler of steps' boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=\"D:/UTD/Summer/Dataset/log/sam\"\n",
    "# DATASET_PATH = \"D:/UTD/Summer/Dataset/\"\n",
    "\n",
    "# np.savez_compressed(DATASET_PATH+f\"original_dataset_{START_DATA}_to_{END_DATA}.npz\", imgs=original_images, masks=original_masks)\n",
    "# np.savez_compressed(DATASET_PATH+f\"almost_preprocessed_dataset_{START_DATA}_to_{END_DATA}.npz\", imgs=original_images, img_inputs=sam_input_images, masks=original_masks)\n",
    "# # np.savez_compressed(DATASET_PATH+f\"preprocessed_dataset_{START_DATA}_to_{END_DATA}.npz\", imgs=original_images, img_embeddings=sam_image_embeddings, masks=original_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
