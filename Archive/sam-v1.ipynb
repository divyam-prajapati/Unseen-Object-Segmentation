{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "import pybullet as p\n",
    "\n",
    "NUM_VIEWS_PER_SCENE = 7\n",
    "\n",
    "BACKGROUND_LABEL = 0\n",
    "TABLE_LABEL = 1\n",
    "OBJECTS_LABEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/chrisdxie/uois/blob/master/src/util/utilities.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def get_color_mask(object_index, nc=None):\n",
    "    \"\"\" Colors each index differently. Useful for visualizing semantic masks\n",
    "\n",
    "        @param object_index: a [H x W] numpy array of ints from {0, ..., nc-1}\n",
    "        @param nc: total number of colors. If None, this will be inferred by masks\n",
    "\n",
    "        @return: a [H x W x 3] numpy array of dtype np.uint8\n",
    "    \"\"\"\n",
    "    object_index = object_index.astype(int)\n",
    "\n",
    "    if nc is None:\n",
    "        NUM_COLORS = object_index.max() + 1\n",
    "    else:\n",
    "        NUM_COLORS = nc\n",
    "\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    colors = [cm(1. * i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "\n",
    "    color_mask = np.zeros(object_index.shape + (3,)).astype(np.uint8)\n",
    "    for i in np.unique(object_index):\n",
    "        if i == 0 or i == -1:\n",
    "            continue\n",
    "        color_mask[object_index == i, :] = np.array(colors[i][:3]) * 255\n",
    "        \n",
    "    return color_mask\n",
    "\n",
    "\n",
    "def build_matrix_of_indices(height, width):\n",
    "    \"\"\" Builds a [height, width, 2] numpy array containing coordinates.\n",
    "\n",
    "        @return: 3d array B s.t. B[..., 0] contains y-coordinates, B[..., 1] contains x-coordinates\n",
    "    \"\"\"\n",
    "    return np.indices((height, width), dtype=np.float32).transpose(1,2,0)\n",
    "\n",
    "\n",
    "def concatenate_spatial_coordinates(feature_map):\n",
    "    \"\"\" Adds x,y coordinates as channels to feature map\n",
    "\n",
    "        @param feature_map: a [T x C x H x W] torch tensor\n",
    "    \"\"\"\n",
    "    T, C, H, W = feature_map.shape\n",
    "\n",
    "    # build matrix of indices. then replicated it T times\n",
    "    MoI = build_matrix_of_indices(H, W) # Shape: [H, W, 2]\n",
    "    MoI = np.tile(MoI, (T, 1, 1, 1)) # Shape: [T, H, W, 2]\n",
    "    MoI[..., 0] = MoI[..., 0] / (H-1) * 2 - 1 # in [-1, 1]\n",
    "    MoI[..., 1] = MoI[..., 1] / (W-1) * 2 - 1\n",
    "    MoI = torch.from_numpy(MoI).permute(0,3,1,2).to(feature_map.device) # Shape: [T, 2, H, W]\n",
    "\n",
    "    # Concatenate on the channels dimension\n",
    "    feature_map = torch.cat([feature_map, MoI], dim=1)\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "def visualize_segmentation(im, masks, nc=None):\n",
    "    \"\"\" Visualize segmentations nicely. Based on code from:\n",
    "        https://github.com/roytseng-tw/Detectron.pytorch/blob/master/lib/utils/vis.py\n",
    "\n",
    "        @param im: a [H x W x 3] RGB image. numpy array of dtype np.uint8\n",
    "        @param masks: a [H x W] numpy array of dtype np.uint8 with values in {0, ..., nc-1}\n",
    "        @param nc: total number of colors. If None, this will be inferred by masks\n",
    "\n",
    "        @return: a [H x W x 3] numpy array of dtype np.uint8\n",
    "    \"\"\" \n",
    "    from matplotlib.patches import Polygon\n",
    "\n",
    "    masks = masks.astype(int)\n",
    "    im = im.copy()\n",
    "\n",
    "    # Generate color mask\n",
    "    if nc is None:\n",
    "        NUM_COLORS = masks.max() + 1\n",
    "    else:\n",
    "        NUM_COLORS = nc\n",
    "\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    colors = [cm(1. * i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "\n",
    "    # Mask\n",
    "    imgMask = np.zeros(im.shape)\n",
    "\n",
    "\n",
    "    # Draw color masks\n",
    "    for i in np.unique(masks):\n",
    "        if i == 0: # background\n",
    "            continue\n",
    "\n",
    "        # Get the color mask\n",
    "        color_mask = np.array(colors[i][:3])\n",
    "        w_ratio = .4\n",
    "        for c in range(3):\n",
    "            color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\n",
    "        e = (masks == i)\n",
    "\n",
    "        # Add to the mask\n",
    "        imgMask[e] = color_mask\n",
    "\n",
    "    # Add the mask to the image\n",
    "    imgMask = (imgMask * 255).round().astype(np.uint8)\n",
    "    im = cv2.addWeighted(im, 0.5, imgMask, 0.5, 0.0)\n",
    "\n",
    "\n",
    "    # Draw mask contours\n",
    "    for i in np.unique(masks):\n",
    "        if i == 0: # background\n",
    "            continue\n",
    "\n",
    "        # Get the color mask\n",
    "        color_mask = np.array(colors[i][:3])\n",
    "        w_ratio = .4\n",
    "        for c in range(3):\n",
    "            color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\n",
    "        e = (masks == i)\n",
    "\n",
    "        # Find contours\n",
    "        contour, hier = cv2.findContours(\n",
    "            e.astype(np.uint8).copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "        # Plot the nice outline\n",
    "        for c in contour:\n",
    "            cv2.drawContours(im, contour, -1, (255,255,255), 2)\n",
    "\n",
    "    return im\n",
    "    \n",
    "\n",
    "### The two functions below were adatped from the DAVIS public dataset ###\n",
    "\n",
    "def imread_indexed(filename):\n",
    "    \"\"\" Load segmentation image (with palette) given filename.\"\"\"\n",
    "    im = Image.open(filename)\n",
    "    annotation = np.array(im)\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def imwrite_indexed(filename,array):\n",
    "    \"\"\" Save indexed png with palette.\"\"\"\n",
    "\n",
    "    palette_abspath = '/home/chrisxie/projects/random_stuff/palette.txt' # hard-coded filepath\n",
    "    color_palette = np.loadtxt(palette_abspath, dtype=np.uint8).reshape(-1,3)\n",
    "\n",
    "    if np.atleast_3d(array).shape[2] != 1:\n",
    "        raise Exception(\"Saving indexed PNGs requires 2D array.\")\n",
    "\n",
    "    im = Image.fromarray(array)\n",
    "    im.putpalette(color_palette.ravel())\n",
    "    im.save(filename, format='PNG')\n",
    "\n",
    "\n",
    "def mask_to_tight_box_numpy(mask):\n",
    "    \"\"\" Return bbox given mask\n",
    "\n",
    "        @param mask: a [H x W] numpy array\n",
    "    \"\"\"\n",
    "    a = np.transpose(np.nonzero(mask))\n",
    "    bbox = np.min(a[:, 1]), np.min(a[:, 0]), np.max(a[:, 1]), np.max(a[:, 0])\n",
    "    return bbox  # x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def mask_to_tight_box_pytorch(mask):\n",
    "    \"\"\" Return bbox given mask\n",
    "\n",
    "        @param mask: a [H x W] torch tensor\n",
    "    \"\"\"\n",
    "    a = torch.nonzero(mask)\n",
    "    bbox = torch.min(a[:, 1]), torch.min(a[:, 0]), torch.max(a[:, 1]), torch.max(a[:, 0])\n",
    "    return bbox  # x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def mask_to_tight_box(mask):\n",
    "    if type(mask) == torch.Tensor:\n",
    "        return mask_to_tight_box_pytorch(mask)\n",
    "    elif type(mask) == np.ndarray:\n",
    "        return mask_to_tight_box_numpy(mask)\n",
    "    else:\n",
    "        raise Exception(f\"Data type {type(mask)} not understood for mask_to_tight_box...\")\n",
    "\n",
    "\n",
    "def compute_xyz(depth_img, camera_params):\n",
    "    \"\"\" Compute ordered point cloud from depth image and camera parameters.\n",
    "        Assumes camera uses left-handed coordinate system, with \n",
    "            x-axis pointing right\n",
    "            y-axis pointing up\n",
    "            z-axis pointing \"forward\"\n",
    "\n",
    "        @param depth_img: a [H x W] numpy array of depth values in meters\n",
    "        @param camera_params: a dictionary with parameters of the camera used \n",
    "\n",
    "        @return: a [H x W x 3] numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute focal length from camera parameters\n",
    "    if 'fx' in camera_params and 'fy' in camera_params:\n",
    "        fx = camera_params['fx']\n",
    "        fy = camera_params['fy']\n",
    "    else: # simulated data\n",
    "        aspect_ratio = camera_params['img_width'] / camera_params['img_height']\n",
    "        e = 1 / (np.tan(np.radians(camera_params['fov']/2.)))\n",
    "        t = camera_params['near'] / e; b = -t\n",
    "        r = t * aspect_ratio; l = -r\n",
    "        alpha = camera_params['img_width'] / (r-l) # pixels per meter\n",
    "        focal_length = camera_params['near'] * alpha # focal length of virtual camera (frustum camera)\n",
    "        fx = focal_length; fy = focal_length\n",
    "\n",
    "    if 'x_offset' in camera_params and 'y_offset' in camera_params:\n",
    "        x_offset = camera_params['x_offset']\n",
    "        y_offset = camera_params['y_offset']\n",
    "    else: # simulated data\n",
    "        x_offset = camera_params['img_width']/2\n",
    "        y_offset = camera_params['img_height']/2\n",
    "\n",
    "    indices = build_matrix_of_indices(camera_params['img_height'], camera_params['img_width'])\n",
    "    indices[..., 0] = np.flipud(indices[..., 0]) # pixel indices start at top-left corner. for these equations, it starts at bottom-left\n",
    "    z_e = depth_img\n",
    "    x_e = (indices[..., 1] - x_offset) * z_e / fx\n",
    "    y_e = (indices[..., 0] - y_offset) * z_e / fy\n",
    "    xyz_img = np.stack([x_e, y_e, z_e], axis=-1) # Shape: [H x W x 3]\n",
    "\n",
    "    return xyz_img\n",
    "\n",
    "\n",
    "def seg2bmap(seg, return_contour=False):\n",
    "    \"\"\" From a segmentation, compute a binary boundary map with 1 pixel wide\n",
    "        boundaries. This boundary lives on the mask, i.e. it's a subset of the mask.\n",
    "\n",
    "        @param seg: a [H x W] numpy array of values in {0,1}\n",
    "\n",
    "        @return: a [H x W] numpy array of values in {0,1}\n",
    "                 a [2 x num_boundary_pixels] numpy array. [0,:] is y-indices, [1,:] is x-indices\n",
    "    \"\"\"\n",
    "    seg = seg.astype(np.uint8)\n",
    "    contours, hierarchy = cv2.findContours(seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    temp = np.zeros_like(seg)\n",
    "    bmap = cv2.drawContours(temp, contours, -1, 1, 1)\n",
    "\n",
    "    if return_contour: # Return the SINGLE largest contour\n",
    "        contour_sizes = [len(c) for c in contours]\n",
    "        ind = np.argmax(contour_sizes)\n",
    "        contour = np.ascontiguousarray(np.fliplr(contours[ind][:,0,:]).T) # Shape: [2 x num_boundary_pixels]\n",
    "        return bmap, contour\n",
    "    else:\n",
    "        return bmap\n",
    "    \n",
    "\n",
    "def largest_connected_component(mask, connectivity=4):\n",
    "    \"\"\" Run connected components algorithm and return mask of largest one\n",
    "\n",
    "        @param mask: a [H x W] numpy array \n",
    "\n",
    "        @return: a [H x W] numpy array of same type as input\n",
    "    \"\"\"\n",
    "\n",
    "    # Run connected components algorithm\n",
    "    num_components, components = cv2.connectedComponents(mask.astype(np.uint8), connectivity=connectivity)\n",
    "\n",
    "    # Find largest connected component via set distance\n",
    "    largest_component_num = -1\n",
    "    largest_component_size = -1 \n",
    "    for j in range(1, num_components):\n",
    "        component_size = np.count_nonzero(components == j)\n",
    "        if component_size > largest_component_size:\n",
    "            largest_component_num = j\n",
    "            largest_component_size = component_size\n",
    "\n",
    "    return (components == largest_component_num).astype(mask.dtype)\n",
    "\n",
    "\n",
    "def torch_to_numpy(torch_tensor, is_standardized_image = False):\n",
    "    \"\"\" Converts torch tensor (NCHW) to numpy tensor (NHWC) for plotting\n",
    "    \n",
    "        If it's an rgb image, it puts it back in [0,255] range (and undoes ImageNet standardization)\n",
    "    \"\"\"\n",
    "    np_tensor = torch_tensor.cpu().clone().detach().numpy()\n",
    "    if np_tensor.ndim == 4: # NCHW\n",
    "        np_tensor = np_tensor.transpose(0,2,3,1)\n",
    "    if is_standardized_image:\n",
    "        _mean=[0.485, 0.456, 0.406]; _std=[0.229, 0.224, 0.225]\n",
    "        for i in range(3):\n",
    "            np_tensor[...,i] *= _std[i]\n",
    "            np_tensor[...,i] += _mean[i]\n",
    "        np_tensor *= 255\n",
    "            \n",
    "    return np_tensor\n",
    "\n",
    "\n",
    "def subplotter(images, titles=None, fig_num=1, plot_width=5):\n",
    "    \"\"\" Function for plotting side-by-side images.\"\"\"\n",
    "\n",
    "    num_images = len(images)\n",
    "    fig = plt.figure(fig_num, figsize=(num_images*plot_width, plot_width))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        if titles:\n",
    "            plt.title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/chrisdxie/uois/blob/master/src/data_augmentation.py\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# My Libraries\n",
    "# from .util import utilities as util_\n",
    "\n",
    "\n",
    "##### Useful Utilities #####\n",
    "\n",
    "def array_to_tensor(array):\n",
    "    \"\"\" Converts a numpy.ndarray (N x H x W x C) to a torch.FloatTensor of shape (N x C x H x W)\n",
    "        OR\n",
    "        converts a nump.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W)\n",
    "    \"\"\"\n",
    "\n",
    "    if array.ndim == 4: # NHWC\n",
    "        tensor = torch.from_numpy(array).permute(0,3,1,2).float()\n",
    "    elif array.ndim == 3: # HWC\n",
    "        tensor = torch.from_numpy(array).permute(2,0,1).float()\n",
    "    else: # everything else\n",
    "        tensor = torch.from_numpy(array).float()\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def translate(img, tx, ty, interpolation=cv2.INTER_LINEAR):\n",
    "    \"\"\" Translate img by tx, ty\n",
    "\n",
    "        @param img: a [H x W x C] image (could be an RGB image, flow image, or label image)\n",
    "    \"\"\"\n",
    "    H, W = img.shape[:2]\n",
    "    M = np.array([[1,0,tx],\n",
    "                  [0,1,ty]], dtype=np.float32)\n",
    "    return cv2.warpAffine(img, M, (W, H), flags=interpolation)\n",
    "\n",
    "def rotate(img, angle, center=None, interpolation=cv2.INTER_LINEAR):\n",
    "    \"\"\" Rotate img <angle> degrees counter clockwise w.r.t. center of image\n",
    "\n",
    "        @param img: a [H x W x C] image (could be an RGB image, flow image, or label image)\n",
    "    \"\"\"\n",
    "    H, W = img.shape[:2]\n",
    "    if center is None:\n",
    "        center = (W//2, H//2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    return cv2.warpAffine(img, M, (W, H), flags=interpolation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Depth augmentations #####\n",
    "\n",
    "def add_noise_to_depth(depth_img, noise_params):\n",
    "    \"\"\" Add noise to depth image. \n",
    "        This is adapted from the DexNet 2.0 code.\n",
    "        Their code: https://github.com/BerkeleyAutomation/gqcnn/blob/75040b552f6f7fb264c27d427b404756729b5e88/gqcnn/sgd_optimizer.py\n",
    "\n",
    "        @param depth_img: a [H x W] set of depth z values\n",
    "    \"\"\"\n",
    "    depth_img = depth_img.copy()\n",
    "\n",
    "    # Multiplicative noise: Gamma random variable\n",
    "    multiplicative_noise = np.random.gamma(noise_params['gamma_shape'], noise_params['gamma_scale'])\n",
    "    depth_img = multiplicative_noise * depth_img\n",
    "\n",
    "    return depth_img\n",
    "\n",
    "def add_noise_to_xyz(xyz_img, depth_img, noise_params):\n",
    "    \"\"\" Add (approximate) Gaussian Process noise to ordered point cloud\n",
    "\n",
    "        @param xyz_img: a [H x W x 3] ordered point cloud\n",
    "    \"\"\"\n",
    "    xyz_img = xyz_img.copy()\n",
    "\n",
    "    H, W, C = xyz_img.shape\n",
    "\n",
    "    # Additive noise: Gaussian process, approximated by zero-mean anisotropic Gaussian random variable,\n",
    "    #                 which is rescaled with bicubic interpolation.\n",
    "    gp_rescale_factor = np.random.randint(noise_params['gp_rescale_factor_range'][0],\n",
    "                                          noise_params['gp_rescale_factor_range'][1])\n",
    "    gp_scale = np.random.uniform(noise_params['gaussian_scale_range'][0],\n",
    "                                 noise_params['gaussian_scale_range'][1])\n",
    "\n",
    "    small_H, small_W = (np.array([H, W]) / gp_rescale_factor).astype(int)\n",
    "    additive_noise = np.random.normal(loc=0.0, scale=gp_scale, size=(small_H, small_W, C))\n",
    "    additive_noise = cv2.resize(additive_noise, (W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    xyz_img[depth_img > 0, :] += additive_noise[depth_img > 0, :]\n",
    "\n",
    "    return xyz_img\n",
    "\n",
    "def dropout_random_ellipses(depth_img, noise_params):\n",
    "    \"\"\" Randomly drop a few ellipses in the image for robustness.\n",
    "        This is adapted from the DexNet 2.0 code.\n",
    "        Their code: https://github.com/BerkeleyAutomation/gqcnn/blob/75040b552f6f7fb264c27d427b404756729b5e88/gqcnn/sgd_optimizer.py\n",
    "\n",
    "        @param depth_img: a [H x W] set of depth z values\n",
    "    \"\"\"\n",
    "    depth_img = depth_img.copy()\n",
    "\n",
    "    # Sample number of ellipses to dropout\n",
    "    num_ellipses_to_dropout = np.random.poisson(noise_params['ellipse_dropout_mean'])\n",
    "\n",
    "    # Sample ellipse centers\n",
    "    nonzero_pixel_indices = np.array(np.where(depth_img > 0)).T # Shape: [#nonzero_pixels x 2]\n",
    "    dropout_centers_indices = np.random.choice(nonzero_pixel_indices.shape[0], size=num_ellipses_to_dropout)\n",
    "    dropout_centers = nonzero_pixel_indices[dropout_centers_indices, :] # Shape: [num_ellipses_to_dropout x 2]\n",
    "\n",
    "    # Sample ellipse radii and angles\n",
    "    x_radii = np.random.gamma(noise_params['ellipse_gamma_shape'], noise_params['ellipse_gamma_scale'], size=num_ellipses_to_dropout)\n",
    "    y_radii = np.random.gamma(noise_params['ellipse_gamma_shape'], noise_params['ellipse_gamma_scale'], size=num_ellipses_to_dropout)\n",
    "    angles = np.random.randint(0, 360, size=num_ellipses_to_dropout)\n",
    "\n",
    "    # Dropout ellipses\n",
    "    for i in range(num_ellipses_to_dropout):\n",
    "        center = dropout_centers[i, :]\n",
    "        x_radius = np.round(x_radii[i]).astype(int)\n",
    "        y_radius = np.round(y_radii[i]).astype(int)\n",
    "        angle = angles[i]\n",
    "\n",
    "        # dropout the ellipse\n",
    "        mask = np.zeros_like(depth_img)\n",
    "        mask = cv2.ellipse(mask, tuple(center[::-1]), (x_radius, y_radius), angle=angle, startAngle=0, endAngle=360, color=1, thickness=-1)\n",
    "        depth_img[mask == 1] = 0\n",
    "\n",
    "    return depth_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### RGB Augmentations #####\n",
    "\n",
    "def standardize_image(image):\n",
    "    \"\"\" Convert a numpy.ndarray [H x W x 3] of images to [0,1] range, and then standardizes\n",
    "\n",
    "        @return: a [H x W x 3] numpy array of np.float32\n",
    "    \"\"\"\n",
    "    image_standardized = np.zeros_like(image).astype(np.float32)\n",
    "\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    for i in range(3):\n",
    "        image_standardized[...,i] = (image[...,i]/255. - mean[i]) / std[i]\n",
    "\n",
    "    return image_standardized\n",
    "\n",
    "def unstandardize_image(image):\n",
    "    \"\"\" Convert a numpy.ndarray [H x W x 3] standardized image back to RGB (type np.uint8)\n",
    "        Inverse of standardize_image()\n",
    "\n",
    "        @return: a [H x W x 3] numpy array of type np.uint8\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    orig_img = (image * std[None,None,:] + mean[None,None,:]) * 255.\n",
    "    return orig_img.round().astype(np.uint8)\n",
    "\n",
    "def random_color_warp(image, d_h=None, d_s=None, d_l=None):\n",
    "    \"\"\" Given an RGB image [H x W x 3], add random hue, saturation and luminosity to the image\n",
    "\n",
    "        Code adapted from: https://github.com/yuxng/PoseCNN/blob/master/lib/utils/blob.py\n",
    "    \"\"\"\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    image_color_warped = np.zeros_like(image)\n",
    "\n",
    "    # Set random hue, luminosity and saturation which ranges from -0.1 to 0.1\n",
    "    if d_h is None:\n",
    "        d_h = (random.random() - 0.5) * 0.2 * 256\n",
    "    if d_l is None:\n",
    "        d_l = (random.random() - 0.5) * 0.2 * 256\n",
    "    if d_s is None:\n",
    "        d_s = (random.random() - 0.5) * 0.2 * 256\n",
    "\n",
    "    # Convert the RGB to HLS\n",
    "    hls = cv2.cvtColor(image.round().astype(np.uint8), cv2.COLOR_RGB2HLS)\n",
    "    h, l, s = cv2.split(hls)\n",
    "\n",
    "    # Add the values to the image H, L, S\n",
    "    new_h = (np.round((h + d_h)) % 256).astype(np.uint8)\n",
    "    new_l = np.round(np.clip(l + d_l, 0, 255)).astype(np.uint8)\n",
    "    new_s = np.round(np.clip(s + d_s, 0, 255)).astype(np.uint8)\n",
    "\n",
    "    # Convert the HLS to RGB\n",
    "    new_hls = cv2.merge((new_h, new_l, new_s)).astype(np.uint8)\n",
    "    new_im = cv2.cvtColor(new_hls, cv2.COLOR_HLS2RGB)\n",
    "\n",
    "    image_color_warped = new_im.astype(np.float32)\n",
    "\n",
    "    return image_color_warped\n",
    "\n",
    "def random_horizontal_flip(image, label):\n",
    "    \"\"\"Randomly horizontally flip the image/label w.p. 0.5\n",
    "\n",
    "        @param image: a [H x W x 3] numpy array\n",
    "        @param label: a [H x W] numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    if random.random() > 0.5:\n",
    "        image = np.fliplr(image).copy()\n",
    "        label = np.fliplr(label).copy()\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "##### Label transformations #####\n",
    "\n",
    "def random_morphological_transform(label, noise_params):\n",
    "    \"\"\" Randomly erode/dilate the label\n",
    "\n",
    "        @param label: a [H x W] numpy array of {0, 1}\n",
    "    \"\"\"\n",
    "\n",
    "    num_tries = 0\n",
    "    valid_transform = False\n",
    "    while not valid_transform:\n",
    "\n",
    "        if num_tries >= noise_params['max_augmentation_tries']:\n",
    "            print('Morph: Exhausted number of augmentation tries...')\n",
    "            return label\n",
    "\n",
    "        # Sample whether we do erosion or dilation, and kernel size for that\n",
    "        x_min, y_min, x_max, y_max = mask_to_tight_box(label)\n",
    "        sidelength = np.mean([x_max - x_min, y_max - y_min])\n",
    "\n",
    "        morphology_kernel_size = 0; num_ksize_tries = 0;\n",
    "        while morphology_kernel_size == 0:\n",
    "            if num_ksize_tries >= 50: # 50 tries for this\n",
    "                print(f'Morph: Exhausted number of augmentation tries... Sidelength: {sidelength}')\n",
    "                return label\n",
    "\n",
    "            dilation_percentage = np.random.beta(noise_params['label_dilation_alpha'], \n",
    "                                                 noise_params['label_dilation_beta'])\n",
    "            morphology_kernel_size = int(round(sidelength * dilation_percentage))\n",
    "\n",
    "            num_ksize_tries += 1\n",
    "\n",
    "        iterations = np.random.randint(1, noise_params['morphology_max_iters']+1)\n",
    "\n",
    "        # Erode/dilate the mask\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (morphology_kernel_size, morphology_kernel_size))\n",
    "        if np.random.rand() < 0.5:\n",
    "            morphed_label = cv2.erode(label, kernel, iterations=iterations)\n",
    "        else:\n",
    "            morphed_label = cv2.dilate(label, kernel, iterations=iterations)\n",
    "\n",
    "        # Make sure there the mass is reasonable\n",
    "        if (np.count_nonzero(morphed_label) / morphed_label.size > 0.001) and \\\n",
    "           (np.count_nonzero(morphed_label) / morphed_label.size < 0.98):\n",
    "            valid_transform = True\n",
    "\n",
    "        num_tries += 1\n",
    "\n",
    "    return morphed_label\n",
    "\n",
    "def random_ellipses(label, noise_params):\n",
    "    \"\"\" Randomly add/drop a few ellipses in the mask\n",
    "        This is adapted from the DexNet 2.0 code.\n",
    "        Their code: https://github.com/BerkeleyAutomation/gqcnn/blob/75040b552f6f7fb264c27d427b404756729b5e88/gqcnn/sgd_optimizer.py\n",
    "\n",
    "        @param label: a [H x W] numpy array of {0, 1}\n",
    "    \"\"\"\n",
    "    H, W = label.shape\n",
    "\n",
    "    num_tries = 0\n",
    "    valid_transform = False\n",
    "    while not valid_transform:\n",
    "\n",
    "        if num_tries >= noise_params['max_augmentation_tries']:\n",
    "            print('Ellipse: Exhausted number of augmentation tries...')\n",
    "            return label\n",
    "\n",
    "        new_label = label.copy()\n",
    "\n",
    "        # Sample number of ellipses to include/dropout\n",
    "        num_ellipses = np.random.poisson(noise_params['num_ellipses_mean'])\n",
    "\n",
    "        # Sample ellipse centers by sampling from Gaussian at object center\n",
    "        pixel_indices = build_matrix_of_indices(H, W)\n",
    "        h_idx, w_idx = np.where(new_label)\n",
    "        mu = np.mean(pixel_indices[h_idx, w_idx, :], axis=0) # Shape: [2]. y_center, x_center\n",
    "        sigma = 2*np.cov(pixel_indices[h_idx, w_idx, :].T) # Shape: [2 x 2]\n",
    "        if np.any(np.isnan(mu)) or np.any(np.isnan(sigma)):\n",
    "            print(mu, sigma, h_idx, w_idx)\n",
    "        ellipse_centers = np.random.multivariate_normal(mu, sigma, size=num_ellipses) # Shape: [num_ellipses x 2]\n",
    "        ellipse_centers = np.round(ellipse_centers).astype(int)\n",
    "\n",
    "        # Sample ellipse radii and angles\n",
    "        x_min, y_min, x_max, y_max = mask_to_tight_box(new_label)\n",
    "        scale_factor = max(x_max - x_min, y_max - y_min) * noise_params['ellipse_size_percentage'] # Mean of gamma r.v.\n",
    "        x_radii = np.random.gamma(noise_params['ellipse_gamma_base_shape'] * scale_factor, \n",
    "                                  noise_params['ellipse_gamma_base_scale'], \n",
    "                                  size=num_ellipses)\n",
    "        y_radii = np.random.gamma(noise_params['ellipse_gamma_base_shape'] * scale_factor, \n",
    "                                  noise_params['ellipse_gamma_base_scale'], \n",
    "                                  size=num_ellipses)\n",
    "        angles = np.random.randint(0, 360, size=num_ellipses)\n",
    "\n",
    "        # Dropout ellipses\n",
    "        for i in range(num_ellipses):\n",
    "            center = ellipse_centers[i, :]\n",
    "            x_radius = np.round(x_radii[i]).astype(int)\n",
    "            y_radius = np.round(y_radii[i]).astype(int)\n",
    "            angle = angles[i]\n",
    "\n",
    "            # include or dropout the ellipse\n",
    "            mask = np.zeros_like(new_label)\n",
    "            mask = cv2.ellipse(mask, tuple(center[::-1]), (x_radius, y_radius), angle=angle, startAngle=0, endAngle=360, color=1, thickness=-1)\n",
    "            if np.random.rand() < 0.5:\n",
    "                new_label[mask == 1] = 0 # Drop out ellipse\n",
    "            else:\n",
    "                new_label[mask == 1] = 1 # Add ellipse\n",
    "\n",
    "        # Make sure the mass is reasonable\n",
    "        if (np.count_nonzero(new_label) / new_label.size > 0.001) and \\\n",
    "           (np.count_nonzero(new_label) / new_label.size < 0.98):\n",
    "            valid_transform = True\n",
    "\n",
    "        num_tries += 1\n",
    "\n",
    "    return new_label\n",
    "\n",
    "def random_translation(label, noise_params):\n",
    "    \"\"\" Randomly translate mask\n",
    "\n",
    "        @param label: a [H x W] numpy array of {0, 1}\n",
    "    \"\"\"\n",
    "\n",
    "    num_tries = 0\n",
    "    valid_transform = False\n",
    "    while not valid_transform:\n",
    "\n",
    "        if num_tries >= noise_params['max_augmentation_tries']:\n",
    "            print('Translate: Exhausted number of augmentation tries...')\n",
    "            return label\n",
    "\n",
    "        # Get tight bbox of mask\n",
    "        x_min, y_min, x_max, y_max = mask_to_tight_box(label)\n",
    "        sidelength = max(x_max - x_min, y_max - y_min)\n",
    "\n",
    "        # sample translation pixels\n",
    "        translation_percentage = np.random.beta(noise_params['translation_alpha'], \n",
    "                                                noise_params['translation_beta'])\n",
    "        translation_percentage = max(translation_percentage, noise_params['translation_percentage_min'])\n",
    "        translation_max = int(round(translation_percentage * sidelength))\n",
    "        translation_max = max(translation_max, 1) # To make sure things don't error out\n",
    "\n",
    "        tx = np.random.randint(-translation_max, translation_max)\n",
    "        ty = np.random.randint(-translation_max, translation_max)   \n",
    "\n",
    "        translated_label = translate(label, tx, ty, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Make sure the mass is reasonable\n",
    "        if (np.count_nonzero(translated_label) / translated_label.size > 0.001) and \\\n",
    "           (np.count_nonzero(translated_label) / translated_label.size < 0.98):\n",
    "            valid_transform = True\n",
    "\n",
    "        num_tries += 1\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def random_rotation(label, noise_params):\n",
    "    \"\"\" Randomly rotate mask\n",
    "\n",
    "        @param label: a [H x W] numpy array of {0, 1}\n",
    "    \"\"\"\n",
    "    H, W = label.shape\n",
    "\n",
    "    num_tries = 0\n",
    "    valid_transform = False\n",
    "    while not valid_transform:\n",
    "\n",
    "        if num_tries >= noise_params['max_augmentation_tries']:\n",
    "            print('Rotate: Exhausted number of augmentation tries...')\n",
    "            return label\n",
    "\n",
    "        # Rotate about center of box\n",
    "        pixel_indices = build_matrix_of_indices(H, W)\n",
    "        h_idx, w_idx = np.where(label)\n",
    "        mean = np.mean(pixel_indices[h_idx, w_idx, :], axis=0) # Shape: [2]. y_center, x_center\n",
    "\n",
    "        # Sample an angle\n",
    "        applied_angle = np.random.uniform(-noise_params['rotation_angle_max'], \n",
    "                                           noise_params['rotation_angle_max'])\n",
    "\n",
    "        rotated_label = rotate(label, applied_angle, center=tuple(mean[::-1]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Make sure the mass is reasonable\n",
    "        if (np.count_nonzero(rotated_label) / rotated_label.size > 0.001) and \\\n",
    "           (np.count_nonzero(rotated_label) / rotated_label.size < 0.98):\n",
    "            valid_transform = True\n",
    "\n",
    "        num_tries += 1\n",
    "\n",
    "    return rotated_label\n",
    "\n",
    "def random_cut(label, noise_params):\n",
    "    \"\"\" Randomly cut part of mask\n",
    "\n",
    "        @param label: a [H x W] numpy array of {0, 1}\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = label.shape\n",
    "\n",
    "    num_tries = 0\n",
    "    valid_transform = False\n",
    "    while not valid_transform:\n",
    "\n",
    "        if num_tries >= noise_params['max_augmentation_tries']:\n",
    "            print('Cut: Exhausted number of augmentation tries...')\n",
    "            return label\n",
    "\n",
    "        cut_label = label.copy()\n",
    "\n",
    "        # Sample cut percentage\n",
    "        cut_percentage = np.random.uniform(noise_params['cut_percentage_min'],\n",
    "                                           noise_params['cut_percentage_max'])\n",
    "\n",
    "        x_min, y_min, x_max, y_max = mask_to_tight_box(label)\n",
    "        if np.random.rand() < 0.5: # choose width\n",
    "            \n",
    "            sidelength = x_max - x_min\n",
    "            if np.random.rand() < 0.5:  # from the left\n",
    "                x = int(round(cut_percentage * sidelength)) + x_min\n",
    "                cut_label[y_min:y_max+1, x_min:x] = 0\n",
    "            else: # from the right\n",
    "                x = x_max - int(round(cut_percentage * sidelength))\n",
    "                cut_label[y_min:y_max+1, x:x_max+1] = 0\n",
    "\n",
    "        else: # choose height\n",
    "            \n",
    "            sidelength = y_max - y_min\n",
    "            if np.random.rand() < 0.5:  # from the top\n",
    "                y = int(round(cut_percentage * sidelength)) + y_min\n",
    "                cut_label[y_min:y, x_min:x_max+1] = 0\n",
    "            else: # from the bottom\n",
    "                y = y_max - int(round(cut_percentage * sidelength))\n",
    "                cut_label[y:y_max+1, x_min:x_max+1] = 0\n",
    "\n",
    "        # Make sure the mass is reasonable\n",
    "        if (np.count_nonzero(cut_label) / cut_label.size > 0.001) and \\\n",
    "           (np.count_nonzero(cut_label) / cut_label.size < 0.98):\n",
    "            valid_transform = True\n",
    "\n",
    "        num_tries += 1\n",
    "\n",
    "    return cut_label\n",
    "\n",
    "\n",
    "def random_add(label, noise_params):\n",
    "    \"\"\" Randomly add part of mask \n",
    "\n",
    "        @param label: a [H x W] numpy array of {0, 1}\n",
    "    \"\"\"\n",
    "    H, W = label.shape\n",
    "\n",
    "    num_tries = 0\n",
    "    valid_transform = False\n",
    "    while not valid_transform:\n",
    "        if num_tries >= noise_params['max_augmentation_tries']:\n",
    "            print('Add: Exhausted number of augmentation tries...')\n",
    "            return label\n",
    "\n",
    "        added_label = label.copy()\n",
    "\n",
    "        # Sample add percentage\n",
    "        add_percentage = np.random.uniform(noise_params['add_percentage_min'],\n",
    "                                           noise_params['add_percentage_max'])\n",
    "\n",
    "        x_min, y_min, x_max, y_max = mask_to_tight_box(label)\n",
    "\n",
    "        # Sample translation from center\n",
    "        translation_percentage_x = np.random.uniform(0, 2*add_percentage)\n",
    "        tx = int(round( (x_max - x_min) * translation_percentage_x ))\n",
    "        translation_percentage_y = np.random.uniform(0, 2*add_percentage)\n",
    "        ty = int(round( (y_max - y_min) * translation_percentage_y ))\n",
    "\n",
    "        if np.random.rand() < 0.5: # choose x direction\n",
    "\n",
    "            sidelength = x_max - x_min\n",
    "            ty = np.random.choice([-1, 1]) * ty # mask will be moved to the left/right. up/down doesn't matter\n",
    "\n",
    "            if np.random.rand() < 0.5: # mask copied from the left. \n",
    "                x = int(round(add_percentage * sidelength)) + x_min\n",
    "                try:\n",
    "                    temp = added_label[y_min+ty : y_max+1+ty, x_min-tx : x-tx]\n",
    "                    added_label[y_min+ty : y_max+1+ty, x_min-tx : x-tx] = np.logical_or(temp, added_label[y_min : y_max+1, x_min : x])\n",
    "                except ValueError as e: # indices were out of bounds\n",
    "                    num_tries += 1\n",
    "                    continue\n",
    "            else: # mask copied from the right\n",
    "                x = x_max - int(round(add_percentage * sidelength))\n",
    "                try:\n",
    "                    temp = added_label[y_min+ty : y_max+1+ty, x+tx : x_max+1+tx]\n",
    "                    added_label[y_min+ty : y_max+1+ty, x+tx : x_max+1+tx] = np.logical_or(temp, added_label[y_min : y_max+1, x : x_max+1])\n",
    "                except ValueError as e: # indices were out of bounds\n",
    "                    num_tries += 1\n",
    "                    continue\n",
    "\n",
    "        else: # choose y direction\n",
    "\n",
    "            sidelength = y_max - y_min\n",
    "            tx = np.random.choice([-1, 1]) * tx # mask will be moved up/down. lef/right doesn't matter\n",
    "\n",
    "            if np.random.rand() < 0.5:  # from the top\n",
    "                y = int(round(add_percentage * sidelength)) + y_min\n",
    "                try:\n",
    "                    temp = added_label[y_min-ty : y-ty, x_min+tx : x_max+1+tx]\n",
    "                    added_label[y_min-ty : y-ty, x_min+tx : x_max+1+tx] = np.logical_or(temp, added_label[y_min : y, x_min : x_max+1])\n",
    "                except ValueError as e: # indices were out of bounds\n",
    "                    num_tries += 1\n",
    "                    continue\n",
    "            else: # from the bottom\n",
    "                y = y_max - int(round(add_percentage * sidelength))\n",
    "                try:\n",
    "                    temp = added_label[y+ty : y_max+1+ty, x_min+tx : x_max+1+tx]\n",
    "                    added_label[y+ty : y_max+1+ty, x_min+tx : x_max+1+tx] = np.logical_or(temp, added_label[y : y_max+1, x_min : x_max+1])\n",
    "                except ValueError as e: # indices were out of bounds\n",
    "                    num_tries += 1\n",
    "                    continue\n",
    "\n",
    "        # Make sure the mass is reasonable\n",
    "        if (np.count_nonzero(added_label) / added_label.size > 0.001) and \\\n",
    "           (np.count_nonzero(added_label) / added_label.size < 0.98):\n",
    "            valid_transform = True\n",
    "\n",
    "        num_tries += 1\n",
    "\n",
    "    return added_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/chrisdxie/uois/blob/master/src/data_loader.py\n",
    "\n",
    "###### Some utilities #####\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\" Use this to bypass issue with PyTorch dataloaders using deterministic RNG for Numpy\n",
    "        https://github.com/pytorch/pytorch/issues/5059\n",
    "    \"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "############# Synthetic Tabletop Object Dataset #############\n",
    "class Tabletop_Object_Dataset(Dataset):\n",
    "    \"\"\" Data loader for Tabletop Object Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, train_or_test, config):\n",
    "        self.base_dir = base_dir\n",
    "        self.config = config\n",
    "        self.train_or_test = train_or_test\n",
    "\n",
    "        # Get a list of all scenes\n",
    "        self.scene_dirs = sorted(glob.glob(self.base_dir + '*/'))\n",
    "        self.len = len(self.scene_dirs) * NUM_VIEWS_PER_SCENE\n",
    "\n",
    "        self.name = 'TableTop'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def process_rgb(self, rgb_img):\n",
    "        \"\"\" Process RGB image\n",
    "                - random color warping\n",
    "        \"\"\"\n",
    "        rgb_img = rgb_img.astype(np.float32)\n",
    "\n",
    "        if self.config['use_data_augmentation']:\n",
    "            # rgb_img = data_augmentation.random_color_warp(rgb_img)\n",
    "            pass\n",
    "        rgb_img = standardize_image(rgb_img)\n",
    "\n",
    "        return rgb_img\n",
    "\n",
    "    def process_bbox(self, bbox, view_num):\n",
    "        # The bounding box needs to be a list of points, corresponding to the flattened coordinates of the top left point, and bottom right point of the bounding box. \n",
    "        if view_num==1:\n",
    "            return [bbox]\n",
    "        return bbox\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        cv2.setNumThreads(0) # some hack to make sure pyTorch doesn't deadlock. Found at https://github.com/pytorch/pytorch/issues/1355. Seems to work for me\n",
    "\n",
    "        # Get scene directory\n",
    "        scene_idx = idx // NUM_VIEWS_PER_SCENE\n",
    "        scene_dir = self.scene_dirs[scene_idx]\n",
    "\n",
    "        # Get view number\n",
    "        view_num = idx % NUM_VIEWS_PER_SCENE\n",
    "\n",
    "        # RGB image\n",
    "        rgb_img_filename = scene_dir + f\"rgb_{view_num:05d}.jpeg\"\n",
    "        rgb_img = cv2.cvtColor(cv2.imread(rgb_img_filename), cv2.COLOR_BGR2RGB)\n",
    "        rgb_img = self.process_rgb(rgb_img)\n",
    "\n",
    "        # Depth image\n",
    "        # depth_img_filename = scene_dir + f\"depth_{view_num:05d}.png\"\n",
    "        # depth_img = cv2.imread(depth_img_filename, cv2.IMREAD_ANYDEPTH) # This reads a 16-bit single-channel image. Shape: [H x W]\n",
    "        # # xyz_img = self.process_depth(depth_img)\n",
    "\n",
    "        # Labels\n",
    "        foreground_labels_filename = scene_dir + f\"segmentation_{view_num:05d}.png\"\n",
    "        foreground_labels = imread_indexed(foreground_labels_filename)\n",
    "        scene_description_filename = scene_dir + \"scene_description.txt\"\n",
    "        scene_description = json.load(open(scene_description_filename))\n",
    "        scene_description['view_num'] = view_num\n",
    "\n",
    "        bbox_filename = scene_dir + f\"bbox_{view_num:05d}.txt\"\n",
    "        bbox_points = np.loadtxt(bbox_filename) # (xmin, ymin, xmax, ymax)\n",
    "        bbox_points = self.process_bbox(bbox_points, view_num) # (xmin, ymin, xmax, ymax) \n",
    "\n",
    "        # center_offset_labels, object_centers = self.process_label_3D(foreground_labels, xyz_img, scene_description)\n",
    "\n",
    "        # label_abs_path = '/'.join(foreground_labels_filename.split('/')[-2:]) # Used for evaluation\n",
    "\n",
    "        # Turn these all into torch tensors\n",
    "        # rgb_img = array_to_tensor(rgb_img) # Shape: [3 x H x W]\n",
    "        # xyz_img = array_to_tensor(xyz_img) # Shape: [3 x H x W]\n",
    "        # foreground_labels = array_to_tensor(foreground_labels) # Shape: [H x W]\n",
    "        # center_offset_labels = array_to_tensor(center_offset_labels) # Shape: [2 x H x W]\n",
    "        # object_centers = array_to_tensor(object_centers) # Shape: [100 x 3]\n",
    "        # num_3D_centers = torch.tensor(np.count_nonzero(np.unique(foreground_labels) >= OBJECTS_LABEL))\n",
    "\n",
    "        return {\n",
    "                'rgb' : rgb_img, # ✅ \n",
    "                # 'xyz' : xyz_img,\n",
    "                'mask' : foreground_labels, # ✅\n",
    "                'bbox': bbox_points, # ✅\n",
    "                # 'center_offset_labels' : center_offset_labels,\n",
    "                # 'object_centers' : object_centers, # This is gonna bug out because the dimensions will be different per frame\n",
    "                # 'num_3D_centers' : num_3D_centers,\n",
    "                # 'scene_dir' : scene_dir,\n",
    "                # 'view_num' : view_num,\n",
    "                # 'label_abs_path' : label_abs_path,\n",
    "            }\n",
    "    \n",
    "def get_TOD_train_dataloader(base_dir, config, batch_size=8, num_workers=4, shuffle=True):\n",
    "\n",
    "    config = config.copy()\n",
    "    dataset = Tabletop_Object_Dataset(base_dir + 'training_set/', 'train', config)\n",
    "\n",
    "    return DataLoader(dataset=dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      num_workers=num_workers,\n",
    "                      worker_init_fn=worker_init_fn)\n",
    "\n",
    "def get_TOD_test_dataloader(base_dir, config, batch_size=8, num_workers=4, shuffle=False):\n",
    "\n",
    "    config = config.copy()\n",
    "    dataset = Tabletop_Object_Dataset(base_dir + 'test_set/', 'test', config)\n",
    "\n",
    "    return DataLoader(dataset=dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      num_workers=num_workers,\n",
    "                      worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOD_filepath = 'C:/dataset/TOD/' # TODO: change this to the dataset you want to train on\n",
    "data_loading_params = {\n",
    "    \n",
    "    # Camera/Frustum parameters\n",
    "    'img_width' : 640, \n",
    "    'img_height' : 480,\n",
    "    'near' : 0.01,\n",
    "    'far' : 100,\n",
    "    'fov' : 45, # vertical field of view in degrees\n",
    "    \n",
    "    'use_data_augmentation' : True,\n",
    "\n",
    "    # Multiplicative noise\n",
    "    'gamma_shape' : 1000.,\n",
    "    'gamma_scale' : 0.001,\n",
    "    \n",
    "    # Additive noise\n",
    "    'gaussian_scale_range' : [0., 0.003], # up to 2.5mm standard dev\n",
    "    'gp_rescale_factor_range' : [12, 20], # [low, high (exclusive)]\n",
    "    \n",
    "    # Random ellipse dropout\n",
    "    'ellipse_dropout_mean' : 10, \n",
    "    'ellipse_gamma_shape' : 5.0, \n",
    "    'ellipse_gamma_scale' : 1.0,\n",
    "\n",
    "    # Random high gradient dropout\n",
    "    'gradient_dropout_left_mean' : 15, \n",
    "    'gradient_dropout_alpha' : 2., \n",
    "    'gradient_dropout_beta' : 5.,\n",
    "\n",
    "    # Random pixel dropout\n",
    "    'pixel_dropout_alpha' : 0.2, \n",
    "    'pixel_dropout_beta' : 10.,\n",
    "    \n",
    "}\n",
    "\n",
    "dataset = Tabletop_Object_Dataset(TOD_filepath + 'training_set/', 'train', data_loading_params)\n",
    "train_dataset = DataLoader(dataset=dataset, batch_size=8, shuffle=True,num_workers=4,worker_init_fn=worker_init_fn)\n",
    "\n",
    "# dl = get_TOD_train_dataloader(TOD_filepath, data_loading_params, batch_size=8, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def show_boxes_on_image(raw_image, boxes):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    for box in boxes:\n",
    "      show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 480, 640) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# seg = visualize_segmentation(rgb_img, torch_to_numpy(mask_img))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\pyplot.py:3343\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3322\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3324\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3341\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3343\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3347\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3348\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3349\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m     sci(__ret)\n\u001b[0;32m   3363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5756\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5756\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5757\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5759\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 480, 640) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rgb_img = dataset.__getitem__(1)['rgb']\n",
    "bbox_points = dataset.__getitem__(1)['bbox']\n",
    "mask_img = dataset.__getitem__(1)['foreground_labels']\n",
    "\n",
    "# seg = visualize_segmentation(rgb_img, torch_to_numpy(mask_img))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(torch_to_numpy(rgb_img))\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "print(type(rgb_img))\n",
    "print(type(torch_to_numpy(rgb_img)))\n",
    "# show_boxes_on_image(torch_to_numpy(rgb_img), bbox_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m view_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m rgb_img_filename \u001b[38;5;241m=\u001b[39m scene_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview_num\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m rgb_img_raw \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mcvtColor(cv2\u001b[38;5;241m.\u001b[39mimread(rgb_img_filename), cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     33\u001b[0m rgb_img \u001b[38;5;241m=\u001b[39m process_rgb(rgb_img_raw)\n\u001b[0;32m     34\u001b[0m rgb_img \u001b[38;5;241m=\u001b[39m array_to_tensor(rgb_img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def array_to_tensor(array):\n",
    "    \"\"\" Converts a numpy.ndarray (N x H x W x C) to a torch.FloatTensor of shape (N x C x H x W)\n",
    "        OR\n",
    "        converts a nump.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W)\n",
    "    \"\"\"\n",
    "\n",
    "    if array.ndim == 4: # NHWC\n",
    "        tensor = torch.from_numpy(array).permute(0,3,1,2).float()\n",
    "    elif array.ndim == 3: # HWC\n",
    "        tensor = torch.from_numpy(array).permute(2,0,1).float()\n",
    "    else: # everything else\n",
    "        tensor = torch.from_numpy(array).float()\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def process_rgb( rgb_img):\n",
    "        \"\"\" Process RGB image\n",
    "                - random color warping\n",
    "        \"\"\"\n",
    "        rgb_img = rgb_img.astype(np.float32)\n",
    "        rgb_img = standardize_image(rgb_img)\n",
    "\n",
    "        return rgb_img\n",
    "\n",
    "scene_dir = \"C:/dataset/TOD/training_set/scene_00000/\"\n",
    "# for view_num in range(7):\n",
    "view_num=0\n",
    "rgb_img_filename = scene_dir + f\"rgb_{view_num:05d}.jpeg\"\n",
    "rgb_img_raw = cv2.cvtColor(cv2.imread(rgb_img_filename), cv2.COLOR_BGR2RGB)\n",
    "rgb_img = process_rgb(rgb_img_raw)\n",
    "rgb_img = array_to_tensor(rgb_img)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(rgb_img)\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# foreground_labels_filename = scene_dir + f\"segmentation_{view_num:05d}.png\"\n",
    "# foreground_labels = imread_indexed(foreground_labels_filename)\n",
    "# seg = visualize_segmentation(rgb_img_raw, foreground_labels)\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(seg)\n",
    "# plt.axis('on')\n",
    "# plt.show()\n",
    "\n",
    "# bbox_filename = scene_dir + f\"bbox_{view_num:05d}.txt\"\n",
    "# bbox_points = np.loadtxt(bbox_filename)\n",
    "# if view_num==1:\n",
    "#     bbox_points = [bbox_points]\n",
    "# show_boxes_on_image(rgb_img_raw, bbox_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision.io import read_image\n",
    "\n",
    "# DATASET_PATH = \"C:/dataset/TOD/training_set\"\n",
    "# DATASET_DIR = os.listdir(DATASET_PATH)\n",
    "# for folder in DATASET_DIR:\n",
    "#     folder_path = os.path.join(DATASET_PATH, folder)\n",
    "#     for i in range(1,7):\n",
    "#         img_path = os.path.join(folder_path, f\"rgb_0000{i}.jpeg\")\n",
    "#         mask_path = os.path.join(folder_path, f\"segmentation_0000{i}.png\")\n",
    "#         bbox_path = os.path.join(folder_path, f\"bbox_0000{i}.txt\")\n",
    "#         img = read_image(img_path)\n",
    "#         mask = read_image(mask_path)\n",
    "#         bbox = np.loadtxt(bbox_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\divya\\.cache\\huggingface\\hub\\models--facebook--sam-vit-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm()\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze The Encoder and Decoder \n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"mask_decoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mmask_decoder\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m      4\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 27/06/2024 AFTERNOON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
